binaries.preprocess.logger.name = preprocess_wmt16_en-ro
binaries.preprocess.logger.off = false
binaries.preprocess.logger.path = preprocess_wmt16_en-ro.log
binaries.preprocess.random_seed = 1234
binaries.preprocess.task = seq2seq
binaries.test.checkpoint_directory = Checkpoints/WMT16_En-Ro
binaries.test.device = GPU
binaries.test.logger.name = test_wmt16_en-ro_newsdev2016
binaries.test.logger.off = false
binaries.test.logger.path = test_wmt16_en-ro_newsdev2016.log
binaries.test.output_directory = Outputs/WMT16_En-Ro
binaries.test.task = seq2seq
binaries.test.tester = seq2seq
binaries.train.checkpoint = 
binaries.train.distribution.device = GPU
binaries.train.distribution.master_ip = 127.0.0.1
binaries.train.distribution.master_port = 23333
binaries.train.distribution.ranks.0 = 0
binaries.train.distribution.ranks.1 = 1
binaries.train.distribution.ranks.2 = 2
binaries.train.distribution.ranks.3 = 3
binaries.train.distribution.workshop_capacity = 60
binaries.train.distribution.world_size = 4
binaries.train.logger.name = train_wmt16_en-ro
binaries.train.logger.off = false
binaries.train.logger.path = train_wmt16_en-ro.log
binaries.train.mix_precision.on = false
binaries.train.mix_precision.optimization_level = O0
binaries.train.model = transformer_base
binaries.train.optimizer.beta1 = 0.9
binaries.train.optimizer.beta2 = 0.98
binaries.train.optimizer.epsilon = 1e-09
binaries.train.optimizer.learning_rate = 0.001
binaries.train.optimizer.name = adam
binaries.train.random_seed = 1234
binaries.train.reset_optimizer = false
binaries.train.reset_scheduler = false
binaries.train.reset_trainer = false
binaries.train.scheduler.name = noam
binaries.train.scheduler.warmup_step = 4000
binaries.train.task = seq2seq
binaries.train.trainer = seq2seq
binaries.train.visualizer.name = seq2seq_wmt16_en-ro
binaries.train.visualizer.off = true
binaries.train.visualizer.offline = true
binaries.train.visualizer.overwrite = true
binaries.train.visualizer.password = Happy_Visual_001
binaries.train.visualizer.path = seq2seq_wmt16_en-ro.vis
binaries.train.visualizer.port = 6789
binaries.train.visualizer.server = www.young-leigh.love
binaries.train.visualizer.username = Guest_Visdom
models.transformer_base.decoder.attention_dropout_probability = 0.1
models.transformer_base.decoder.dimension = 512
models.transformer_base.decoder.dropout_probability = 0.1
models.transformer_base.decoder.feedforward_dimension = 2048
models.transformer_base.decoder.feedforward_dropout_probability = 0.1
models.transformer_base.decoder.head_number = 8
models.transformer_base.decoder.layer_number = 6
models.transformer_base.decoder.normalize_position = before
models.transformer_base.dimension = 512
models.transformer_base.encoder.attention_dropout_probability = 0.1
models.transformer_base.encoder.dimension = 512
models.transformer_base.encoder.dropout_probability = 0.1
models.transformer_base.encoder.feedforward_dimension = 2048
models.transformer_base.encoder.feedforward_dropout_probability = 0.1
models.transformer_base.encoder.head_number = 8
models.transformer_base.encoder.layer_number = 6
models.transformer_base.encoder.normalize_position = before
models.transformer_base.name = transformer
models.transformer_base.share_dec_io_embeddings = true
models.transformer_base.share_enc_dec_embeddings = true
tasks.seq2seq.datasets.training = Datasets/wmt16.en-ro.dataset
tasks.seq2seq.datasets.validation = Datasets/newsdev2016.en-ro.dataset
tasks.seq2seq.datasets.vocabularies = Datasets/wmt16.en-ro.vocab
tasks.seq2seq.language.source = 'en'
tasks.seq2seq.language.target = 'ro'
tasks.seq2seq.name = seq2seq
tasks.seq2seq.number_worker = 16
tasks.seq2seq.raw_data.training.source = Corpora/wmt16.en-ro.en
tasks.seq2seq.raw_data.training.target = Corpora/wmt16.en-ro.ro
tasks.seq2seq.raw_data.validation.source = Corpora/newsdev2016.en-ro.en
tasks.seq2seq.raw_data.validation.target = Corpora/newsdev2016.en-ro.ro
tasks.seq2seq.shard_size = 1000000
tasks.seq2seq.training_batches.batch_size = 4096
tasks.seq2seq.training_batches.batch_type = token
tasks.seq2seq.training_batches.dock_size = 8192
tasks.seq2seq.training_batches.export_volume = 2
tasks.seq2seq.training_batches.filter.source.0 = 0
tasks.seq2seq.training_batches.filter.source.1 = 200
tasks.seq2seq.training_batches.filter.target.0 = 0
tasks.seq2seq.training_batches.filter.target.1 = 200
tasks.seq2seq.training_batches.shuffle = true
tasks.seq2seq.validation_batches.batch_size = 32
tasks.seq2seq.validation_batches.batch_type = sentence
tasks.seq2seq.vocabularies.share = true
tasks.seq2seq.vocabularies.size_limit.source = 32768
tasks.seq2seq.vocabularies.size_limit.target = 32768
tasks.seq2seq.work_amount = 300000
testers.seq2seq.batch_size = 80
testers.seq2seq.batch_type = sentence
testers.seq2seq.bpe_symbol = @@ 
testers.seq2seq.name = seq2seq
testers.seq2seq.remove_bpe = true
testers.seq2seq.searcher.beam_size = 4
testers.seq2seq.searcher.max_length = 200
testers.seq2seq.searcher.min_length = 1
testers.seq2seq.searcher.n_best = 1
testers.seq2seq.searcher.name = beam
testers.seq2seq.searcher.penalty.alpha = 0.6
testers.seq2seq.searcher.penalty.beta = 0.0
testers.seq2seq.source = Corpora/newsdev2016.en-ro.en
testers.seq2seq.target = Corpora/Gold/newsdev2016.en-ro.ro
trainers.seq2seq.checkpoints.directory = Checkpoints/WMT16_En-Ro
trainers.seq2seq.checkpoints.keep_number = 100
trainers.seq2seq.checkpoints.name = seq2seq_wmt16_en-ro
trainers.seq2seq.life_cycle = 30000
trainers.seq2seq.name = seq2seq
trainers.seq2seq.normalization_type = token
trainers.seq2seq.report_period = 1
trainers.seq2seq.training_criterion.label_smoothing_percent = 0.1
trainers.seq2seq.training_criterion.name = label_smoothing_cross_entropy
trainers.seq2seq.training_period = 1000
trainers.seq2seq.validation_criterion.name = cross_entropy
trainers.seq2seq.validation_period = 1000