binaries.preprocess.logger.name = preprocess_wmt16_en-de
binaries.preprocess.logger.off = false
binaries.preprocess.logger.path = preprocess_wmt16_en-de.log
binaries.preprocess.random_seed = 1234
binaries.preprocess.task = seq2seq
binaries.test.checkpoint_directory = Checkpoints/WMT16_En-De
binaries.test.device = GPU
binaries.test.logger.name = test_wmt16_en-de_newstest2013
binaries.test.logger.off = false
binaries.test.logger.path = test_wmt16_en-de_newstest2013.log
binaries.test.output_directory = Outputs/WMT16_En-De
binaries.test.task = seq2seq
binaries.test.tester = seq2seq
binaries.train.checkpoint = 
binaries.train.distribution.device = GPU
binaries.train.distribution.master_ip = 127.0.0.1
binaries.train.distribution.master_port = 12345
binaries.train.distribution.ranks.0 = 0
binaries.train.distribution.ranks.1 = 1
binaries.train.distribution.ranks.2 = 2
binaries.train.distribution.ranks.3 = 3
binaries.train.distribution.ranks.4 = 4
binaries.train.distribution.ranks.5 = 5
binaries.train.distribution.ranks.6 = 6
binaries.train.distribution.ranks.7 = 7
binaries.train.distribution.workshop_capacity = 80
binaries.train.distribution.world_size = 8
binaries.train.logger.name = train_wmt16_en-de
binaries.train.logger.off = false
binaries.train.logger.path = train_wmt16_en-de.log
binaries.train.mix_precision.on = false
binaries.train.mix_precision.optimization_level = O0
binaries.train.model = transformer_base
binaries.train.optimizer.beta1 = 0.9
binaries.train.optimizer.beta2 = 0.98
binaries.train.optimizer.epsilon = 1e-09
binaries.train.optimizer.learning_rate = 0.001
binaries.train.optimizer.name = adam
binaries.train.random_seed = 1234
binaries.train.reset_optimizer = false
binaries.train.reset_scheduler = false
binaries.train.reset_trainer = false
binaries.train.scheduler.name = noam
binaries.train.scheduler.warmup_step = 4000
binaries.train.task = seq2seq
binaries.train.trainer = seq2seq
binaries.train.visualizer.name = seq2seq_wmt16_en-de
binaries.train.visualizer.off = true
binaries.train.visualizer.offline = true
binaries.train.visualizer.overwrite = true
binaries.train.visualizer.password = Happy_Visual_001
binaries.train.visualizer.path = seq2seq_wmt16_en-de.vis
binaries.train.visualizer.port = 6789
binaries.train.visualizer.server = www.young-leigh.love
binaries.train.visualizer.username = Guest_Visdom
models.transformer_base.decoder.attention_dropout_probability = 0.1
models.transformer_base.decoder.dimension = 512
models.transformer_base.decoder.dropout_probability = 0.1
models.transformer_base.decoder.feedforward_dimension = 2048
models.transformer_base.decoder.feedforward_dropout_probability = 0.1
models.transformer_base.decoder.head_number = 8
models.transformer_base.decoder.layer_number = 6
models.transformer_base.decoder.normalize_position = before
models.transformer_base.dimension = 512
models.transformer_base.encoder.attention_dropout_probability = 0.1
models.transformer_base.encoder.dimension = 512
models.transformer_base.encoder.dropout_probability = 0.1
models.transformer_base.encoder.feedforward_dimension = 2048
models.transformer_base.encoder.feedforward_dropout_probability = 0.1
models.transformer_base.encoder.head_number = 8
models.transformer_base.encoder.layer_number = 6
models.transformer_base.encoder.normalize_position = before
models.transformer_base.name = transformer
models.transformer_base.share_dec_io_embeddings = true
models.transformer_base.share_enc_dec_embeddings = true
tasks.seq2seq.datasets.training = Datasets/wmt16.en-de.dataset
tasks.seq2seq.datasets.validation = Datasets/newstest2013.en-de.dataset
tasks.seq2seq.datasets.vocabularies = Datasets/wmt16.en-de.vocab
tasks.seq2seq.language.source = 'en'
tasks.seq2seq.language.target = 'de'
tasks.seq2seq.name = seq2seq
tasks.seq2seq.number_worker = 16
tasks.seq2seq.raw_data.training.source = Corpora/wmt16.en-de.en
tasks.seq2seq.raw_data.training.target = Corpora/wmt16.en-de.de
tasks.seq2seq.raw_data.validation.source = Corpora/newstest2013.en-de.en
tasks.seq2seq.raw_data.validation.target = Corpora/newstest2013.en-de.de
tasks.seq2seq.shard_size = 1000000
tasks.seq2seq.training_batches.batch_size = 4096
tasks.seq2seq.training_batches.batch_type = token
tasks.seq2seq.training_batches.dock_size = 8192
tasks.seq2seq.training_batches.export_volume = 1
tasks.seq2seq.training_batches.filter.source.0 = 0
tasks.seq2seq.training_batches.filter.source.1 = 200
tasks.seq2seq.training_batches.filter.target.0 = 0
tasks.seq2seq.training_batches.filter.target.1 = 200
tasks.seq2seq.training_batches.shuffle = true
tasks.seq2seq.validation_batches.batch_size = 32
tasks.seq2seq.validation_batches.batch_type = sentence
tasks.seq2seq.vocabularies.share = true
tasks.seq2seq.vocabularies.size_limit.source = 32768
tasks.seq2seq.vocabularies.size_limit.target = 32768
tasks.seq2seq.work_amount = 1000000
testers.seq2seq.batch_size = 80
testers.seq2seq.batch_type = sentence
testers.seq2seq.bpe_symbol = @@ 
testers.seq2seq.name = seq2seq
testers.seq2seq.remove_bpe = true
testers.seq2seq.searcher.beam_size = 4
testers.seq2seq.searcher.max_length = 200
testers.seq2seq.searcher.min_length = 1
testers.seq2seq.searcher.n_best = 1
testers.seq2seq.searcher.name = beam
testers.seq2seq.searcher.penalty.alpha = 0.6
testers.seq2seq.searcher.penalty.beta = 0.0
testers.seq2seq.source = Corpora/newstest2013.en-de.en
testers.seq2seq.target = Corpora/Gold/newstest2013.en-de.de
trainers.seq2seq.checkpoints.directory = Checkpoints/WMT16_En-De
trainers.seq2seq.checkpoints.keep_number = 100
trainers.seq2seq.checkpoints.name = seq2seq_wmt16_en-de
trainers.seq2seq.life_cycle = 180000
trainers.seq2seq.name = seq2seq
trainers.seq2seq.normalization_type = token
trainers.seq2seq.report_period = 1
trainers.seq2seq.training_criterion.label_smoothing_percent = 0.1
trainers.seq2seq.training_criterion.name = label_smoothing_cross_entropy
trainers.seq2seq.training_period = 1000
trainers.seq2seq.validation_criterion.name = cross_entropy
trainers.seq2seq.validation_period = 1000